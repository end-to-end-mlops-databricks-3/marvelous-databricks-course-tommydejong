{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf83b5b-97a9-43fa-a48a-d23e44708632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install -e ..\n",
    "%pip install git+https://github.com/end-to-end-mlops-databricks-3/marvelous@0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4599ef9-65ab-46eb-a14f-e6e69e444306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A better approach (this file must be present in a notebook folder, achieved via synchronization)\n",
    "%pip install ../dist/fifa_players-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5bf02b-3ce6-4953-abd4-ca619469b651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#restart python\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae18bd4-185d-476b-b870-639b53de700f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# system path update, must be after %restart_python\n",
    "# caution! This is not a great approach\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671ddb7e-3b22-4783-88fa-ad29897cace0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "from fifa_players import __version__\n",
    "from fifa_players.config import ProjectConfig\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlflow.models import infer_signature\n",
    "from marvelous.common import is_databricks\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from mlflow import MlflowClient\n",
    "import pandas as pd\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from databricks import feature_engineering\n",
    "from databricks.feature_engineering import FeatureFunction, FeatureLookup\n",
    "from pyspark.errors import AnalysisException\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d668dc8-29a6-46cc-8fdf-b62b0eb2763d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not is_databricks():\n",
    "    load_dotenv()\n",
    "    profile = os.environ[\"PROFILE\"]\n",
    "    mlflow.set_tracking_uri(f\"databricks://{profile}\")\n",
    "    mlflow.set_registry_uri(f\"databricks-uc://{profile}\")\n",
    "\n",
    "\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\", env=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0563d3-135b-4eed-a0be-f3ff4d3bb9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "fe = feature_engineering.FeatureEngineeringClient()\n",
    "\n",
    "train_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.train_set\")\n",
    "test_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ed12b0-37f3-4320-9b69-ad5d8d068635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature table with information about players\n",
    "\n",
    "feature_table_name = f\"{config.catalog_name}.{config.schema_name}.player_features\"\n",
    "lookup_features = [\"overall_rating\", \"potential\", \"release_clause_euro\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0881256a-9ee6-4106-9e64-31e2d5f6fb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table = fe.create_table(\n",
    "   name=feature_table_name,\n",
    "   primary_keys=[\"Id\"],\n",
    "   df=train_set[[\"Id\"]+lookup_features],\n",
    "   description=\"Player features table\",\n",
    ")\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "fe.write_table(\n",
    "   name=feature_table_name,\n",
    "   df=test_set[[\"Id\"]+lookup_features],\n",
    "   mode=\"merge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf034afa-747d-49ff-9968-d36c5af20088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature table with information about players\n",
    "# Option 2: SQL\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE OR REPLACE TABLE {feature_table_name}\n",
    "          (Id STRING NOT NULL, overall_rating INT, potential INT, release_clause_euro INT);\n",
    "          \"\"\")\n",
    "# primary key on Databricks is not enforced!\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {feature_table_name} ADD CONSTRAINT house_pk_demo PRIMARY KEY(Id);\")\n",
    "except AnalysisException:\n",
    "    pass\n",
    "spark.sql(f\"ALTER TABLE {feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\")\n",
    "spark.sql(f\"\"\"\n",
    "          INSERT INTO {feature_table_name}\n",
    "          SELECT Id, overall_rating, potential, release_clause_euro\n",
    "          FROM {config.catalog_name}.{config.schema_name}.train_set\n",
    "          \"\"\")\n",
    "spark.sql(f\"\"\"\n",
    "          INSERT INTO {feature_table_name}\n",
    "          SELECT Id, overall_rating, potential, release_clause_euro\n",
    "          FROM {config.catalog_name}.{config.schema_name}.test_set\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5b6164-97f8-4478-a415-270881c042d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature function\n",
    "# docs: https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function\n",
    "\n",
    "# problems with feature functions:\n",
    "# functions are not versioned \n",
    "# functions may behave differently depending on the runtime (and version of packages and python)\n",
    "# there is no way to enforce python version & package versions for the function \n",
    "# this is only supported from runtime 17\n",
    "# advised to use only for simple calculations\n",
    "\n",
    "function_name = f\"{config.catalog_name}.{config.schema_name}.calculate_potential_ratio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827ff230-c29a-404a-82f8-1d77864b0ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Option 1: with Python\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {function_name} (rating INT, future_potential INT)\n",
    "        RETURNS INT\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        return rating / future_potential\n",
    "        $$\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c7bbfc-b2b7-42df-a046-63a7a184919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# it is possible to define simple functions in sql only without python\n",
    "# Option 2\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {function_name}_sql (rating INT, future_potential INT)\n",
    "        RETURNS INT\n",
    "        RETURN rating / future_potential;\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9decc0c-23f8-4350-bcb6-eab73ece85cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# execute function\n",
    "spark.sql(f\"SELECT {function_name}_sql(65, 76) as potential_ratio;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2bc62c-2aa7-4e5c-9206-a3e5b46a0004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a training set\n",
    "training_set = fe.create_training_set(\n",
    "    df=train_set.drop(\"overall_rating\", \"potential\", \"release_clause_euro\"),\n",
    "    label=config.target,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            feature_names=[\"overall_rating\", \"potential\", \"release_clause_euro\"],\n",
    "            lookup_key=\"Id\",\n",
    "                ),\n",
    "        FeatureFunction(\n",
    "            udf_name=function_name,\n",
    "            output_name=\"potential_ratio\",\n",
    "            input_bindings={\n",
    "                \"rating\": \"overall_rating\",\n",
    "                \"future_potential\": \"potential\"\n",
    "                },\n",
    "            ),\n",
    "    ],\n",
    "    exclude_columns=[\"update_timestamp_utc\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5fef43c-4c73-42d1-b5ea-08c6a4c272eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train & register a model\n",
    "training_df = training_set.load_df().toPandas()\n",
    "X_train = training_df[config.num_features + config.cat_features + [\"potential_ratio\"]]\n",
    "y_train = training_df[config.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dac9c79-d244-43d2-959c-ae1d50f91d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "        steps=[(\"preprocessor\", ColumnTransformer(\n",
    "            transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                           config.cat_features)],\n",
    "            remainder=\"passthrough\")\n",
    "            ),\n",
    "               (\"regressor\", LGBMRegressor(**config.parameters))]\n",
    "        )\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1059af09-5905-414b-a344-dd9b4c69a2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Shared/fifa-players-model-fe\")\n",
    "with mlflow.start_run(run_name=\"fifa-players-model-fe\",\n",
    "                      tags={\"git_sha\": \"77889944bc\",\n",
    "                            \"branch\": \"feat/week3\"},\n",
    "                            description=\"run for feature engineering of potential ratio of fifa players\") as run:\n",
    "    # Log parameters and metrics\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(config.parameters)\n",
    "\n",
    "    # Log the model\n",
    "    signature = infer_signature(model_input=X_train, model_output=pipeline.predict(X_train))\n",
    "    fe.log_model(\n",
    "                model=pipeline,\n",
    "                flavor=mlflow.sklearn,\n",
    "                artifact_path=\"lightgbm-pipeline-model-fe\",\n",
    "                training_set=training_set,\n",
    "                signature=signature,\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd55aae-1980-423d-9a9f-497e336a62ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = f\"{config.catalog_name}.{config.schema_name}.fifa_players_model_fe_demo\"\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f'runs:/{run_id}/lightgbm-pipeline-model-fe',\n",
    "    name=model_name,\n",
    "    tags={\"git_sha\": \"77889944bc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8faace55-8af2-4e60-9914-33499fd808f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "features = [f for f in [\"Id\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set[features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5b3321-5ba6-42b0-a320-515b05376b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62def06-d15f-4394-a65f-298631215c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "features = [f for f in [\"Id\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "test_set_with_new_id = test_set.select(*features).withColumn(\n",
    "    \"Id\",\n",
    "    (col(\"Id\").cast(\"long\") + 1000000).cast(\"string\")\n",
    ")\n",
    "\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set_with_new_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf2aaf2-6191-4404-8254-e3d0d0c3694f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "overallrating_function = f\"{config.catalog_name}.{config.schema_name}.replace_overallrating_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {overallrating_function}(overall_rating INT)\n",
    "        RETURNS INT\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if overall_rating is None:\n",
    "            return 5\n",
    "        else:\n",
    "            return overall_rating\n",
    "        $$\n",
    "        \"\"\")\n",
    "\n",
    "potential_function = f\"{config.catalog_name}.{config.schema_name}.replace_potential_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {potential_function}(potential INT)\n",
    "        RETURNS INT\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if potential is None:\n",
    "            return 1000\n",
    "        else:\n",
    "            return potential\n",
    "        $$\n",
    "        \"\"\")\n",
    "\n",
    "relclause_function = f\"{config.catalog_name}.{config.schema_name}.replace_relclause_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {relclause_function}(release_clause_euro INT)\n",
    "        RETURNS INT\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if release_clause_euro is None:\n",
    "            return 2\n",
    "        else:\n",
    "            return release_clause_euro\n",
    "        $$\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c41b562-9406-4269-87db-98f1a7927b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# what if we want to replace with a default value if entry is not found\n",
    "# what if we want to look up value in another table? the logics get complex\n",
    "# problems that arize: functions/ lookups always get executed (if statememt is not possible)\n",
    "# it can get slow...\n",
    "\n",
    "# step 1: create 3 feature functions\n",
    "\n",
    "# step 2: redefine create training set\n",
    "\n",
    "# try again\n",
    "\n",
    "# create a training set\n",
    "training_set = fe.create_training_set(\n",
    "    df=train_set.drop(\"overall_rating\", \"potential\", \"release_clause_euro\"),\n",
    "    label=config.target,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            feature_names=[\"overall_rating\", \"potential\", \"release_clause_euro\"],\n",
    "            lookup_key=\"Id\",\n",
    "            rename_outputs={\"overall_rating\": \"lookup_overall_rating\",\n",
    "                            \"potential\": \"lookup_potential\",\n",
    "                            \"release_clause_euro\": \"lookup_release_clause_euro\"}\n",
    "                ),\n",
    "        FeatureFunction(\n",
    "            udf_name=overallrating_function,\n",
    "            output_name=\"overall_rating\",\n",
    "            input_bindings={\"overall_rating\": \"lookup_overall_rating\"},\n",
    "            ),\n",
    "        FeatureFunction(\n",
    "            udf_name=potential_function,\n",
    "            output_name=\"potential\",\n",
    "            input_bindings={\"potential\": \"lookup_potential\"},\n",
    "        ),\n",
    "        FeatureFunction(\n",
    "            udf_name=relclause_function,\n",
    "            output_name=\"release_clause_euro\",\n",
    "            input_bindings={\"release_clause_euro\": \"lookup_release_clause_euro\"},\n",
    "        ),\n",
    "        FeatureFunction(\n",
    "            udf_name=function_name,\n",
    "            output_name=\"potential_ratio\",\n",
    "            input_bindings={\n",
    "                \"rating\": \"overall_rating\",\n",
    "                \"future_potential\": \"potential\"\n",
    "                },\n",
    "            ),\n",
    "    ],\n",
    "    exclude_columns=[\"update_timestamp_utc\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b42dfb-9ecb-4ada-8434-e02afe1bfef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train & register a model\n",
    "training_df = training_set.load_df().toPandas()\n",
    "X_train = training_df[config.num_features + config.cat_features + [\"potential_ratio\"]]\n",
    "y_train = training_df[config.target]\n",
    "\n",
    "#pipeline\n",
    "pipeline = Pipeline(\n",
    "        steps=[(\"preprocessor\", ColumnTransformer(\n",
    "            transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                           config.cat_features)],\n",
    "            remainder=\"passthrough\")\n",
    "            ),\n",
    "               (\"regressor\", LGBMRegressor(**config.parameters))]\n",
    "        )\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ec76dd-cd32-4725-8e17-bdd348bb2067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Shared/fifa-players-model-fe\")\n",
    "with mlflow.start_run(run_name=\"fifa-players-model-fe\",\n",
    "                      tags={\"git_sha\": \"77889944bc\",\n",
    "                            \"branch\": \"feat/week3\"},\n",
    "                            description=\"run for feature engineering of potential ratio of fifa players\") as run:\n",
    "    # Log parameters and metrics\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(config.parameters)\n",
    "\n",
    "    # Log the model\n",
    "    signature = infer_signature(model_input=X_train, model_output=pipeline.predict(X_train))\n",
    "    fe.log_model(\n",
    "                model=pipeline,\n",
    "                flavor=mlflow.sklearn,\n",
    "                artifact_path=\"lightgbm-pipeline-model-fe\",\n",
    "                training_set=training_set,\n",
    "                signature=signature,\n",
    "            )\n",
    "model_name = f\"{config.catalog_name}.{config.schema_name}.model_fe_demo\"\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f'runs:/{run_id}/lightgbm-pipeline-model-fe',\n",
    "    name=model_name,\n",
    "    tags={\"git_sha\": \"77889944bc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe685bf-01de-44ac-be8f-0c1c1c848d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "features = [f for f in [\"Id\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "test_set_with_new_id = test_set.select(*features).withColumn(\n",
    "    \"Id\",\n",
    "    (col(\"Id\").cast(\"long\") + 1000000).cast(\"string\")\n",
    ")\n",
    "\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set_with_new_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164deeaa-8e72-40f4-9fed-4bc975a87741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions for a non-existing entry -> no error!\n",
    "predictions.select(\"prediction\").show(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "week3_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
